# Microsoft-Malware-Prediction

## Table of Contents
1. [Overview](#1-overview)
2. [IBM Data Science Methodology](#2-ibm-data-science-methodology)
3. [Solution Implementation](#3-solution-implementation)
    - [3.1 Business Understanding](#31-business-understanding)
    - [3.2 Analytic Approach](#32-analytic-approach)
    - [3.3 Data Requirements](#33-data-requirements)
    - [3.4 Data Collection](#34-data-collection)
    - [3.5 Data Understanding](#35-data-understanding)
    - [3.6 Data Preparation](#36-data-preparation)
    - [3.7 Modeling](#37-modeling)
    - [3.8 Evaluation](#38-evaluation)
    - [3.9 Deployment](#39-deployment)
    - [3.10 Feedback](#310-feedback)
4. [Requirements](#4-requirements)
5. [Usage](#5-usage)
6. [Reflections](#6-reflections)
7. [Links](#7-links)
8. [Appendix](#8-appendix)
    - [8.1 Appendix A: Understanding Label Encoding and Frequency Encoding](#81-appendix-a-understanding-label-encoding-and-frequency-encoding)

Microsoft Malware Detection EDA + XGBoost
https://www.kaggle.com/code/praxitelisk/microsoft-malware-detection-eda-xgboost


# 1. Overview
This repository documents my journey and learning from participating in the [Microsoft Malware Prediction competition on Kaggle](https://www.kaggle.com/c/microsoft-malware-prediction). The goal of this competition was to predict the probability that a machine running Windows 10 could be susceptible to malware attacks, based on various system features provided by Microsoft.

- **Emulating Real-World Solutions:** The approach focused on creating a model that could realistically be implemented in a production environment, not just optimized for competition scores.
- **Handling Large Datasets:** Developed strategies for managing and processing large datasets efficiently to prepare the data for modeling.


## 2. IBM Data Science Methodology

This project follows the structured approach outlined in the IBM Data Science Methodology, a comprehensive framework designed to guide data science projects from conception to deployment. The methodology ensures that each stage of the project aligns with business objectives and incorporates rigorous data analysis and model evaluation.

### Highlights of the Methodology Applied:
- **Business Understanding:** We defined the cybersecurity threats and objectives clearly at the start, focusing on identifying and mitigating potential breaches.
- **Analytic Approach:** Our choice of machine learning models was guided by the need for real-time anomaly detection and threat analysis.
- **Data Management:** Extensive data preparation was undertaken to ensure that the input data supported the objectives set out in the initial phase of the project.

For a detailed exploration of each step of the IBM Data Science Methodology and its general application, please refer to our comprehensive report [here](link-to-detailed-methodology-report).

### Application in Our Project:
- Each section of this report aligns with a phase in the IBM methodology, from data collection and preparation to modeling and feedback. Specific details on the application of each phase are discussed in their respective sections.

This approach not only keeps your report streamlined but also enriches it with strategic touchpoints linking back to the foundational methodology, ensuring coherence and depth without redundancy.

<img src="img/datascience_methodology_flowchart.png" alt="IBM" width="600">

*Figure 2: IBM Data Science Methodology flowchart*

# 3. Solution Summary

## 3.1 Business Understanding

**Definition:**  
The primary business objective was to predict which machines would likely encounter malware infections. This predictive capability was essential for developing proactive measures to protect users' systems and reduce the impact of malware attacks. The problem was framed as a classification task where the goal was to accurately identify machines that were at risk of malware infections.

**Goals:**  
- Reduce the incidence of malware infections by predicting and addressing potential threats.
- Develop a robust predictive model that could be integrated into security software to enhance real-time protection.
- Improve user experience by minimizing the disruptions caused by malware.

**Implementation:**  
We engaged with stakeholders to define specific project goals and success metrics. Key performance indicators (KPIs) included prediction accuracy, precision, recall, and the reduction in malware incidents post-implementation. Detailed meetings and reviews were conducted to align the project objectives with the business needs, ensuring that the developed model would provide actionable insights and enhance overall system security.

## 3.2 Analytic Approach

**Definition:**  
To address the business problem of predicting malware occurrences, we employed state-of-the-art machine learning techniques. The focus was on developing a highly accurate predictive model capable of identifying potential malware threats based on system and software configurations.

**Approach:**  
We utilized supervised learning techniques, specifically classification algorithms, to distinguish between malware-infected and clean machines. Given the complexity and scale of the data, advanced ensemble methods were selected for their robustness and performance in high-dimensional data scenarios.

**Model Selection:**  
We began with exploratory data analysis (EDA) to understand the underlying patterns and distributions in the dataset. Based on insights from EDA, we experimented with various machine learning algorithms, including:

- **Gradient Boosting Machines (GBM):** Known for their accuracy and ability to handle large datasets with complex relationships.
- **Random Forests:** Provided robustness against overfitting and were effective in handling imbalanced data.
- **LightGBM:** An optimized gradient boosting framework that uses tree-based learning algorithms, suitable for large datasets, offering fast training speeds and high efficiency.

**Feature Engineering:**  
To enhance model performance, we performed extensive feature engineering, creating new features that captured interactions and complexities within the data, such as:

- **Interaction Features:** Captured relationships between different system and software attributes.
- **Aggregation Features:** Summarized important metrics and reduced dimensionality.
- **Encoding Techniques:** Transformed categorical variables into a suitable format for machine learning models.

**Implementation:**  
We implemented the chosen models using Python libraries such as scikit-learn, LightGBM, and XGBoost. The process included:

1. **Data Preprocessing:** Cleaning and transforming the data to ensure high quality and consistency.
2. **Model Training:** Training various models using cross-validation to ensure robust performance.
3. **Hyperparameter Tuning:** Employing techniques such as Bayesian optimization to fine-tune model parameters for optimal performance.
4. **Model Evaluation:** Assessing models using metrics like accuracy, precision, recall, and AUC to ensure they met the business objectives.

By systematically applying these analytic approaches, we developed a predictive model that significantly enhanced the detection and prevention of malware threats on Windows operating systems.

## 3.3 Data Requirements

**Definition:**  
The data requirements phase involved identifying and collecting the necessary data to address the problem of predicting which machines would likely encounter malware infections. The goal was to gather comprehensive telemetry data that included various properties of machines and their infection statuses.

**Data Sources:**  
The dataset used for this project was provided by Microsoft as part of the Microsoft Malware Prediction competition on Kaggle. The dataset comprised three main files:
- `train.csv`: Training data containing machine properties and malware detection labels.
- `test.csv`: Test data for which predictions needed to be made.
- `sample_submission.csv`: A sample submission file demonstrating the expected format for predictions.

**Dataset Description:**  
The dataset included telemetry data generated by combining heartbeat and threat reports collected by Microsoft's Windows Defender. Each row corresponded to a machine uniquely identified by a `MachineIdentifier`, with `HasDetections` serving as the ground truth label indicating malware detection on the machine. 

**Implementation:**  
To ensure data availability and integrity, we collaborated with IT and security teams to verify the data sources and understand the sampling methodology used. This collaboration ensured that the dataset met privacy constraints and represented a diverse range of machines, albeit with a higher proportion of malware-infected machines than typically found in the wild.

**Columns in the Dataset:**
- **MachineIdentifier:** Unique ID for each machine.
- **ProductName:** Defender state information, e.g., win8defender.
- **EngineVersion:** Version of the Defender engine.
- **AppVersion:** Version of the Defender app.
- **AvSigVersion:** Version of the Defender antivirus signatures.
- **IsBeta:** Indicates if the machine is running a beta version of the software.
- **DefaultBrowsersIdentifier:** ID for the machine's default browser.
- **AVProductStatesIdentifier:** ID for the antivirus product configuration.
- **CountryIdentifier:** ID for the country where the machine is located.
- **OrganizationIdentifier:** ID for the organization the machine belongs to.
- **GeoNameIdentifier:** ID for the geographic region of the machine.
- **Platform:** Platform name calculated from OS and processor properties.
- **Processor:** Processor architecture of the installed operating system.
- **OsVer:** Version of the current operating system.
- **OsBuild:** Build number of the current operating system.
- **OsSuite:** Product suite mask for the current operating system.
- **OsPlatformSubRelease:** OS Platform sub-release (e.g., Windows 7, Windows 8).
- **SkuEdition:** SKU-Edition name useful in population reporting.
- **IsProtected:** Indicates if the machine is protected by an active and up-to-date antivirus product.
- **AutoSampleOptIn:** Indicates if the machine is set to automatically submit samples.
- **PuaMode:** Indicates if Potentially Unwanted Applications mode is enabled.
- **SMode:** Indicates if the machine is in 'S Mode' where only Microsoft Store apps can be installed.
- **SmartScreen:** SmartScreen enabled status from the registry.
- **Firewall:** Indicates if Windows firewall is enabled.
- **UacLuaenable:** UAC (User Account Control) status for admin approval mode.
- **Census_MDC2FormFactor:** Device form factor based on hardware characteristics.
- **Census_DeviceFamily:** Type of device for the OS edition (e.g., Windows.Desktop).
- **Census_ProcessorCoreCount:** Number of logical processor cores.
- **Census_PrimaryDiskTotalCapacity:** Total capacity of the primary disk in MB.
- **Census_PrimaryDiskTypeName:** Type of primary disk (e.g., HDD, SSD).
- **Census_SystemVolumeTotalCapacity:** Size of the partition that the system volume is installed on in MB.
- **Census_HasOpticalDiskDrive:** Indicates if the machine has an optical disk drive.
- **Census_TotalPhysicalRAM:** Total physical RAM in MB.
- **Census_InternalPrimaryDiagonalDisplaySizeInInches:** Diagonal length of the primary display in inches.
- **Census_InternalPrimaryDisplayResolutionHorizontal:** Horizontal resolution of the internal display.
- **Census_InternalPrimaryDisplayResolutionVertical:** Vertical resolution of the internal display.
- **Census_PowerPlatformRoleName:** OEM preferred power management profile.
- **Census_InternalBatteryType:** Type of internal battery.
- **Census_InternalBatteryNumberOfCharges:** Number of charges of the internal battery.
- **Census_OSVersion:** Numeric OS version.
- **Census_OSArchitecture:** OS architecture (e.g., amd64).
- **Census_OSBranch:** Branch of the OS.
- **Census_OSBuildNumber:** OS build number.
- **Census_OSBuildRevision:** OS build revision.
- **Census_OSEdition:** Edition of the current OS.
- **Census_OSSkuName:** OS edition friendly name.
- **Census_OSInstallTypeName:** Description of the install type (e.g., clean).
- **Census_OSInstallLanguageIdentifier:** OS install language identifier.
- **Census_OSUILocaleIdentifier:** OS UI locale identifier.
- **Census_OSWUAutoUpdateOptionsName:** Windows Update auto-update settings name.
- **Census_IsPortableOperatingSystem:** Indicates if the OS is running from a USB stick.
- **Census_GenuineStateName:** OS genuine state.
- **Census_ActivationChannel:** Activation channel (e.g., retail, volume).
- **Census_IsFlightingInternal:** Indicates if the machine is part of internal testing.
- **Census_IsFlightsDisabled:** Indicates if the machine is not receiving flights.
- **Census_FlightRing:** The ring for receiving flights.
- **Census_ThresholdOptIn:** Indicates if the machine is opted into threshold flights.
- **Census_FirmwareManufacturerIdentifier:** Firmware manufacturer ID.
- **Census_FirmwareVersionIdentifier:** Firmware version ID.
- **Census_IsSecureBootEnabled:** Indicates if Secure Boot is enabled.
- **Census_IsWIMBootEnabled:** Indicates if WIMBoot is enabled.
- **Census_IsVirtualDevice:** Indicates if the machine is a virtual device.
- **Census_IsTouchEnabled:** Indicates if the device is touch-enabled.
- **Census_IsPenCapable:** Indicates if the device supports pen input.
- **Census_IsAlwaysOnAlwaysConnectedCapable:** Indicates if the device supports always-on connectivity.
- **Wdft_IsGamer:** Indicates if the device is identified as a gaming device.
- **Wdft_RegionIdentifier:** Region identifier for the device.

This comprehensive dataset allowed us to explore a wide range of machine properties and their correlations with malware infections, providing a robust foundation for building predictive models.

## 3.4 Data Collection

**Definition:**
The data collection phase involved gathering the necessary data from various sources to build a robust dataset for training and testing our predictive models. For this project, the primary data source was the dataset provided by Microsoft for the Microsoft Malware Prediction competition on Kaggle.

**Implementation:**
We utilized ETL (Extract, Transform, Load) techniques to efficiently gather, transform, and load the data into a format suitable for analysis. The following steps outline the key processes involved in data collection:

1. **Importing Libraries and Declaring Functions:**
   We imported necessary libraries such as pandas, numpy, and dask, and declared functions to optimize the data types for reduced memory usage.

2. **Loading and Cleaning Data:**
   - **Data Types Optimization:** To ensure efficient memory usage, we converted data types of the dataset's features. This included converting object types to categories and numerics to appropriate integer or float types.
   - **Loading Data:** We used dask to load the large dataset quickly. The train and test data files, `train.csv` and `test.csv`, were read into dataframes.
   - **Feature Selection:** We selected relevant features from the dataset based on a predefined list of important features. This step involved reading an Excel file that contained feature descriptions and filtering out irrelevant features.
   - **Handling Missing Values:** We deleted features with too many NaN values and those with highly unbalanced dimensions to improve the quality of the data.

3. **Data Processing:**
   - **Combining Datasets:** We combined train and test datasets after converting the data types to ensure consistency.
   - **Saving Processed Data:** The cleaned and optimized data was saved to new CSV files for further analysis and modeling.

By following these steps, we ensured that the collected data was of high quality and suitable for building effective predictive models. This thorough data collection process laid a solid foundation for subsequent stages of the data science project, including data preprocessing, feature engineering, and modeling.

## 3.5 Data Understanding

**Definition:**
The data understanding phase involves exploring the dataset to gain insights into its properties, including quality, completeness, and relevance. This is crucial for identifying potential data quality issues and understanding the distributions and relationships between variables.

**Implementation:**
During this phase, we performed several key tasks to thoroughly understand the dataset and its characteristics:

### 3.5.1 Checking for Missing Data
- **Findings:** 
  - Some features, such as `PuaMode`, `Census_ProcessorClass`, and `DefaultBrowsersIdentifier`, had over 70% missing values.
  - These features were analyzed to determine their significance and the impact of their missing values on the model.

### 3.5.2 Exploratory Data Analysis (EDA)
#### Categorical Features Analysis
- **Findings:**
  - The distribution of `SmartScreen` and its correlation with the target variable `HasDetections` showed notable differences in malware detection rates.
  - Features like `Census_OSEdition` and `Census_OSVersion` also revealed significant patterns.

#### Numerical Features Analysis
- **Findings:**
  - Numerical features such as `Census_ProcessorCoreCount` showed that higher core counts correlated with higher malware detection rates. The top 10 values counts for `Census_ProcessorCoreCount` revealed that the most common configurations were 4 cores (912,463 instances) and 2 cores (388,477 instances), with a maximum value of 88 cores.
  - The `Census_TotalPhysicalRAM` feature revealed different patterns in RAM sizes between infected and non-infected machines.
- **Screenshot Suggestion:** KDE plots and histograms for `Census_ProcessorCoreCount` and `Census_TotalPhysicalRAM` with `HasDetections`.


### 3.5.3 Correlation Analysis
- **Findings:**
  - Correlation analysis highlighted that `Census_InternalPrimaryDiagonalDisplaySizeInInches` and `Census_TotalPhysicalRAM` had significant correlations with the target variable.
  - Strong correlations between certain features suggested potential multicollinearity.
- **Screenshot Suggestion:** Heatmap of the correlation matrix for the top 20 features.

### 3.5.4 Feature Evaluation
- **Findings:**
  - Features with high missing values like `Census_InternalBatteryType` were grouped into broader categories to evaluate their relevance.
  - `DefaultBrowsersIdentifier` was found to have minimal impact on the target variable and was subsequently removed.
- **Screenshot Suggestion:** Count plots for `Census_InternalBatteryType` after grouping and analysis of `DefaultBrowsersIdentifier`.

By incorporating these detailed analyses and visualizations, we gained a deep understanding of the dataset, allowing us to make informed decisions in the subsequent phases of the data science process.

### Screenshots and Visuals

#### Missing Data
<img src="img/features_with_high_nan.png" alt="nan_features" width="900"> 
*Figure 1: Percentage of missing values for the top 10 features.*

#### Categorical Features
SmartScreen                                           | Census_OSEdition
:---------------------------------------------------: | :---------------------------------------------------:
<img src="img/SmartScreen.png" width="450">  | <img src="img/Census_OSEdition.png" alt="HTML Table Target" width="440"> 
*Figure 2: Distribution of `SmartScreen` with `Census_OSEdition`.*


#### Numerical Features

|Census_ProcessorCoreCount|
KDE Plot + Histogram                                        | Census_OSEdition
:---------------------------------------------------: | :---------------------------------------------------:
<img src="img/KDE_Census_ProcessorCoreCount.png" width="650">  | <img src="img/Census_ProcessorCoreCount.png" alt="HTML Table Target" width="400"> 
*Figure 2: Distribution of `SmartScreen` with `Census_OSEdition`.*


![Processor Core Count](images/processor_core_count.png)
*Figure 4: KDE plot for `Census_ProcessorCoreCount` with `HasDetections`.*

![Total Physical RAM](images/total_physical_ram.png)
*Figure 5: KDE plot for `Census_TotalPhysicalRAM` with `HasDetections`.*

#### Correlation Analysis
![Correlation Heatmap](images/correlation_heatmap.png)
*Figure 6: Heatmap of the correlation matrix for the top 20 features.*

#### Feature Evaluation
![Battery Type Grouping](images/battery_type_grouping.png)
*Figure 7: Count plot for `Census_InternalBatteryType` after grouping.*

![Default Browsers Identifier](images/default_browsers_identifier.png)
*Figure 8: Analysis of `DefaultBrowsersIdentifier` impact on `HasDetections`.*

By including these images and findings, we provide concrete evidence of our data understanding process, making it easier for readers to follow along and grasp the significance of our analysis.




## 3.1 Data Preprocessing

### 3.1.1 Introduction
Data preprocessing is a critical step in the data science workflow, especially in the context of cybersecurity. It involves cleaning and transforming raw data into a format that is suitable for analysis and modeling. This section will discuss various data preprocessing techniques used in this project, explaining their importance in a way that is accessible to non-technical business executives.

### 3.1.2 Data Cleaning
The Data Cleaning Process is essential for ensuring the quality and usability of the data. Here's an overview of the key steps involved and their significance:

#### Feature Removal
**Missing Values:** Features with too many NaN (missing) values are deleted because they can skew the analysis and might lead to unreliable models. A threshold of 90% missing values is generally considered as a criterion for removal, as it indicates that the feature is largely absent and likely not useful for predictive modeling.

<img src="img/features_with_high_nan.png" alt="nan_features" width="900"> 

*Figure 1: Missing Values - This table shows the featurew with the highest percentages of missing values.  For example: PuaMode and Census_ProcessorClass have 99%+ missing values.*

**Unbalanced Features:** Features with highly unbalanced dimensions are deleted. For example, a feature where a single category dominates can be less informative for modeling. A common threshold for considering a feature as unbalanced is when more than 95% of the values are concentrated in one category. This high concentration means the feature will not contribute significantly to the model's ability to distinguish between different observations.

Unbalanced Featues Table                              | Unbalanced IsBeta Feature
:---------------------------------------------------: | :---------------------------------------------------:
<img src="img/unbalanced_features.png" alt="HTML Table Target" width="450">  | <img src="img/umbalanced_feature_bar_chart.png" alt="HTML Table Target" width="610"> 

*Figure 2: Unbalanced Features - There are 26 columns in total in which one category contains > or = 90% of the values (left) and esentially 100% of IsBeta values belong to IsBeta (right). *

### 3.1.3 Model 1 Feature Engineering
Proper categorization and encoding of features are crucial as they directly influence the accuracy and efficiency of predictive models. They ensure that the algorithm correctly interprets the data, respects the nature of the data, and helps in achieving better performance. Features are split into three main types: binary, numeric, and categorical. Each type requires specific handling to ensure optimal data structure for machine learning algorithms:

#### General Data Type Encoding Guidance

**Binary Features:**
- **Definition:** Features with only two possible values (e.g., 0 or 1, Yes or No).
- **Encoding Best Practices:** Binary features are often left as-is if they are already in numeric format (0 and 1). If binary features are in text format (e.g., Yes/No), they should be encoded to 0 and 1 respectively.
- **Importance:** Proper handling ensures that models interpret these features correctly without additional transformation complexity.

**Numeric Features:**
- **Definition:** Features with quantitative values that can be measured.
- **Encoding Best Practices:** Numeric features may be scaled or normalized to ensure that no single feature dominates the model due to its scale.
- **Importance:** Scaling or normalizing numeric features helps in speeding up the learning process of algorithms and improves model performance.

**Categorical Features:**
- **Definition:** Features with a fixed number of categories or distinct groups.
- **Encoding Best Practices:** Categorical variables are typically encoded using techniques such as one-hot encoding, label encoding, or using embedding layers if using neural networks.
- **Importance:** Encoding transforms categorical data into a format that can be easily used by machine learning algorithms to better understand the patterns within the data.

#### Encoding Methods Used
- **Label Encoding:** Transforms categorical feature labels into numeric form ranging from `0` to `N-1`, where `N` is the number of different labels. This is typically used for categorical features that have a natural ordinal relationship.

- **Frequency Encoding:** A variant of label encoding where features are encoded based on the frequency of their values. The encoded numbers range from `0` to `N`, where `N` is the number of values that occur more than once. This method is effective for categorical data where the frequency of values may have a predictive power.

- [Appendix A: Understanding Label Encoding and Frequency Encoding](#81-appendix-a-understanding-label-encoding-and-frequency-encoding)


### 3.1.4 Model 2 Feature Engineering

#### Introduction
In developing our second model, M2, for cybersecurity threat detection, we have employed a variety of advanced feature engineering techniques. These techniques not only enhance the model's predictive power but also ensure that it can effectively handle the complexities associated with cybersecurity data. The feature engineering process involved transforming raw data into more informative features that improve detection capabilities and model robustness.

#### Specific Techniques Employed

**Domain-Specific Features**:
- **Primary Drive C Ratio**: This feature represents the ratio of the system volume total capacity to the primary disk total capacity. It is a domain-specific feature that helps in assessing the usage pattern of the primary drive which might be indicative of unusual behaviors if the ratios are outliers.
- **Non-Primary Drive MB**: Represents the unused capacity of the primary disk. This feature is useful for identifying potential red flags where the primary disk usage is inconsistent with typical user behavior.

**Polynomial and Interaction Features**:
- **Aspect Ratio and Screen Area**: These features are derived by interacting screen resolution dimensions and diagonal display size. They represent the physical characteristics of the display used by the system, which can be crucial for identifying fraudulent activities where display resolution manipulation is common.
- **ProcessorCoreCount_DisplaySizeInInches**: An interaction feature that multiplies the processor core count with the diagonal display size, capturing a compound metric that could be critical in systems analysis within cybersecurity contexts.

**Aggregation Features**:
- **Ram Per Processor**: Represents the average RAM available per processor core, calculated by dividing the total physical RAM by the number of processor cores. This feature aggregates system specifications into a single metric that could be indicative of performance capabilities or potential bottlenecks.

**Encoding Techniques**:
- **Monitor Dims and SmartScreen_AVProductsInstalled**: Both features utilize categorical encoding to transform text descriptions into a machine-readable format. `Monitor Dims` captures the combined dimensions of the monitor, while `SmartScreen_AVProductsInstalled` combines the SmartScreen settings with the number of installed antivirus products for a nuanced look at system security posture.
  - **Label Encoding**: Applied to `Monitor Dims` to convert dimensions into a numeric format that preserves the ordinal nature without increasing dimensionality.
  - **Frequency Encoding**: Used for `SmartScreen_AVProductsInstalled` to reflect the prevalence of each configuration within the dataset, providing a weight to more common or rare configurations.
#### Integration into Datasets
The new features were integrated into the existing datasets and stored accordingly:
- **Training Data**: `../data/train_featureengineering_M2.csv`
- **Testing Data**: `../data/test_featureengineering_M2.csv`

These features were encoded and included in the model training process for M2, aimed at improving the predictive accuracy by providing new angles and information vectors. The integration process involved careful handling of categorical variables and normalization of numerical features to align with machine learning algorithms' requirements.

#### Feature Encoding Strategy
The newly created features required appropriate encoding to ensure they are correctly interpreted by the machine learning algorithms:
- **Label Encoding**: Applied to features like `monitor_dims` and `SmartScreen_AVProductsInstalled`, converting string labels into integers.
- **Categorical Encoding**: Used for features that represent categories with significant ordinal relationships and require model understanding of the hierarchy within the data.

### Summary
Feature engineering is a critical step in enhancing model performance. By thoughtfully creating and integrating new features, we aim to capture more complexity and provide our models with a deeper understanding of the data, which is crucial for detecting and predicting cybersecurity threats more effectively.

## 3.2 Modeling

### 3.2.1 M1
**Model M1 Summary:**
Model M1 implements the [LightGBM](https://github.com/rexcoleman/Data-Science-Model-Selection-in-Cybersecurity/blob/main/README.md#lightgbm) method, leveraging its high efficiency with large datasets and categorical features. The model training involved setting hyperparameters that were optimized through Bayesian Hyperparameter Optimization using a 3-fold Cross Validation to ensure robustness and high performance.

**Model Training and Validation:**
The model was trained using a variety of features selected based on their importance and relevance to the task. After training, the model's performance was validated using data from the test set, ensuring that it generalizes well to new, unseen data. The features used were sorted by their importance, which was crucial for understanding the driving factors behind the model's predictions.

**Technical Details:**
- **Training Data:** Data was prepared and encoded appropriately before training to handle different types of features effectively.
- **Hyperparameters:** Tuned using Bayesian optimization to find the best combination that maximizes the model's accuracy.
- **Validation:** Employed a 3-fold Cross Validation strategy during the tuning phase to validate the model's effectiveness across different subsets of the data.

**Model Saving and Prediction:**
The trained model was saved to disk, allowing for reproducibility and later use in operational settings. Predictions were made on the test set, and the output was prepared for submission, showcasing the practical application of the model in a competitive scenario.

**Feature Importance:**
A comprehensive list of features sorted by their importance was generated, providing insights into what factors most significantly impact malware detection. This analysis helps in refining the model further by focusing on the most impactful features.

**Outputs:**
- Model file: `../models/model_M1.p`
- Feature importance file: `../feature_importances/FeatureImportance_M1.csv`
- Submission file: `../submissions/Submission_M1.csv`

### 3.2.1.1 Hyperparameter Settings Used:
The following hyperparameters were finely tuned to optimize the LightGBM model for the Microsoft Malware Prediction competition:

- **Boosting Type**: 'gbdt' (Gradient Boosting Decision Tree)
- **Class Weight**: None
- **Colsample by Tree**: 0.611
- **Learning Rate**: 0.0106
- **Min Child Samples**: 295
- **Num Leaves**: 160
- **Reg Alpha**: 0.632
- **Reg Lambda**: 0.631
- **Subsample for Bin**: 80,000
- **Subsample**: 0.820

### 3.2.1.2 Impact of Each Hyperparameter on Model Performance
Understanding how each hyperparameter influences the model can help in fine-tuning and achieving better performance:

- **Boosting Type**: Specifies the type of algorithm to run. 'gbdt' is the standard gradient boosting decision tree, known for its effectiveness in classification tasks.
- **Class Weight**: Used to handle imbalanced classes. If not specified, all classes are supposed to have weight one.
- **Colsample by Tree**: Fraction of features (columns) to be used per tree. Reducing this can prevent overfitting.
- **Learning Rate**: Determines the step size at each iteration while moving toward a minimum of a loss function. Lower rates require more trees but can lead to better accuracy.
- **Min Child Samples**: Minimum number of data points in a leaf. If this is set too low, the model might overfit.
- **Num Leaves**: The maximum number of leaves in one tree. More leaves will increase accuracy but may cause overfitting.
- **Reg Alpha** (L1 regularization): This is used on weights. Increasing this value will make the model more conservative.
- **Reg Lambda** (L2 regularization): Similar to L1 but differently influences the magnitude of weights. It effectively reduces overfitting.
- **Subsample for Bin**: Number of subsamples to use for constructing bins. Affects the speed of the construction algorithm.
- **Subsample**: Ratio of the training instance. Setting it to 0.8 means LightGBM will randomly sample 80% of the data for building trees and can help in preventing overfitting.

### 3.2.1.3 Additional Hyperparameters to Consider
Exploring more hyperparameters could further enhance model performance:

- **Max Depth**: Limits the number of nodes in the tree. Tune it from 3 to 10 for different depth levels to control over-fitting.
- **Feature Fraction**: Try values between 0.5 to 0.8 to give a random subset of features for each tree to train on, which can help in improving accuracy and control overfitting.
- **Bagging Frequency**: Specifies how often bagging is used. Increasing this number could be beneficial if subsample < 1.
- **Early Stopping Rounds**: Specify a number of iterations over which, if performance does not improve, training will stop. This can prevent overfitting and reduce computational waste.

These additional settings can be explored using grid or random search to find the best combination that improves the overall accuracy and robustness of the model in detecting malware.




## 3.3 Validation Strategy
- Ensured a robust validation strategy that mimics real-world deployment rather than optimizing for leaderboard position, emphasizing the importance of model generalization.

# 4. Requirements
- **Python Version:** 3.7.1
- **Major Libraries:** pandas, numpy, lightgbm, scikit-learn

# 5. Usage
1. **Data Preparation:** Run the notebooks under `notebooks/` to clean and prepare data.
2. **Model Training:** Execute model training scripts to train and save the model.
3. **Evaluation:** Evaluate the model's performance on a separate validation set to ensure generalizability.

# 6. Reflections
Participating in this competition offered invaluable insights into the practical aspects of machine learning projects, including the importance of data quality, thoughtful feature engineering, and the need for robust validation methods.

# 7. Links
- [Competition Data](https://www.kaggle.com/c/microsoft-malware-prediction/data)
- [My Kaggle Profile](https://www.kaggle.com/yourusername)

# 8. Appendix

## 8.1 Appendix A: Understanding Label Encoding and Frequency Encoding

When preparing data for machine learning models, particularly those that handle categorical data, it is crucial to convert the data into a format that can be effectively processed by algorithms. Label encoding and frequency encoding are two popular methods for transforming categorical data into numerical data, each serving distinct purposes and suitable for different scenarios. This section outlines these encoding techniques, providing insights into when and why each method should be used.

### Label Encoding
Label encoding is a straightforward method where each unique category value is assigned a unique integer. For example, if a feature such as `Color` has three categories: Red, Yellow, and Blue, label encoding would replace them with 0, 1, and 2 respectively. This method is efficient in terms of memory and simplicity, making it a common first choice for transforming categorical data into a usable format for algorithms that require numerical input, such as logistic regression, support vector machines, and other linear models.

**Advantages of Label Encoding:**
1. **Simplicity:** It is straightforward to implement and doesn't increase the data dimensionality.
2. **Efficiency:** Requires less computational power and memory, which makes it suitable for large datasets.

**Disadvantages of Label Encoding:**
1. **Ordinality:** Label encoding can introduce an artificial order or hierarchy among categories, where none exists. For instance, assigning Yellow = 1 and Red = 2 might lead the model to believe that Red is greater than Yellow, which might not be a meaningful or desired interpretation.
2. **Limited Application:** Not suitable for models that assume a natural ordering between categories, such as tree-based models, unless the ordinal relationship is intended.

### Frequency Encoding
Frequency encoding, a variant of label encoding, assigns values to categories based on their frequencies. This method counts how many times each category appears in the dataset and then replaces the categories with these counts. This encoding method is particularly useful when the frequency of the category itself might be a predictive signal. For instance, in fraud detection, more frequent categories might be more closely associated with specific outcomes.

**Advantages of Frequency Encoding:**
1. **Preserves Information:** By encoding categories based on their frequencies, it retains information about the dataset's distribution that could be useful for some models.
2. **Non-linear Relationships:** More suitable for non-linear models that can understand and leverage the frequency-based ordering of categories.

**Disadvantages of Frequency Encoding:**
1. **Collisions:** Different categories might end up with the same frequency, thus receiving the same encoding. This can cause a loss of information as distinct categories are treated similarly.
2. **Overfitting:** Models might overfit to the frequency of categories seen in the training data, which might not generalize well to unseen categories or different distributions in the test set.

### When to Use Each Method
Choosing between label encoding and frequency encoding often depends on the nature of the data and the type of model being used:

- **Label Encoding** is generally better when:
  - The categorical feature is ordinal.
  - The number of categories is quite large as it does not expand the feature space.
  - The learning algorithm you are using does not assume any order (e.g., tree-based methods).

- **Frequency Encoding** works well when:
  - The frequency of occurrence of category values is important for the prediction.
  - The model can handle non-linear relationships well, like random forests or gradient boosting machines.

In practice, the choice of encoding might also depend on experimentation. It is often a good idea to try multiple encoding methods during the feature engineering phase and compare their performance using cross-validation. This empirical approach helps identify which method works best for the specific characteristics of the data and the predictive model.


