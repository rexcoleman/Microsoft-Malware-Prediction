# Data Science Portfolio: Cybersecurity Focus

## Table of Contents
1. [Overview](#1-overview)
2. [Key Lessons Learned](#2-key-lessons-learned)
3. [Solution Summary](#3-solution-summary)
    - [3.1 Data Preprocessing](#31-data-preprocessing)
        - [3.1.1 Introduction](#311-introduction)
        - [3.1.2 Data Cleaning](#312-data-cleaning)
    - [3.2 Modeling](#32-modeling)
    - [3.3 Validation Strategy](#33-validation-strategy)
4. [Requirements](#4-requirements)
5. [Usage](#5-usage)
6. [Reflections](#6-reflections)
7. [Links](#7-links)

# 1. Overview
This repository documents my journey and learning from participating in the [Microsoft Malware Prediction competition on Kaggle](https://www.kaggle.com/c/microsoft-malware-prediction). The goal of this competition was to predict the probability that a machine running Windows 10 could be susceptible to malware attacks, based on various system features provided by Microsoft.

# 2. Key Lessons Learned
- **Emulating Real-World Solutions:** The approach focused on creating a model that could realistically be implemented in a production environment, not just optimized for competition scores.
- **Handling Large Datasets:** Developed strategies for managing and processing large datasets efficiently to prepare the data for modeling.

# 3. Solution Summary

## 3.1 Data Preprocessing

### 3.1.1 Introduction
Data preprocessing is a critical step in the data science workflow, especially in the context of cybersecurity. It involves cleaning and transforming raw data into a format that is suitable for analysis and modeling. This section will discuss various data preprocessing techniques used in this project, explaining their importance in a way that is accessible to non-technical business executives.

### 3.1.2 Data Cleaning
The Data Cleaning Process is essential for ensuring the quality and usability of the data. Here's an overview of the key steps involved and their significance:

#### Feature Removal
**Handling Missing Values:** Features with too many NaN (missing) values are deleted because they can skew the analysis and might lead to unreliable models. A threshold of 90% missing values is generally considered as a criterion for removal, as it indicates that the feature is largely absent and likely not useful for predictive modeling.

<img src="img/features_with_high_nan.png" alt="nan_features" width="900"> 

*Figure 1: Missing Values - This table shows the featurew with the highest percentages of missing values.  For example: PuaMode and Census_ProcessorClass have 99%+ missing values.*

**Unbalanced Features:** Features with highly unbalanced dimensions are deleted. For example, a feature where a single category dominates can be less informative for modeling. A common threshold for considering a feature as unbalanced is when more than 95% of the values are concentrated in one category. This high concentration means the feature will not contribute significantly to the model's ability to distinguish between different observations.

Unbalanced Featues Table                              | Unbalanced IsBeta Feature
:---------------------------------------------------: | :---------------------------------------------------:
<img src="img/unbalanced_features.png" alt="HTML Table Target" width="450">  | <img src="img/umbalanced_feature_bar_chart.png" alt="HTML Table Target" width="610"> 

*Figure 2: Unbalanced Features - There are 26 columns in total in which one category contains > or = 90% of the values (left) and esentially 100% of IsBeta values belong to IsBeta (right). *

#### Feature Categorization
Proper categorization and encoding of features are essential as they directly influence the accuracy and efficiency of predictive models. They ensure that the algorithm correctly interprets the data, respects the nature of the data, and helps in achieving better performance.  Features are split into three main types: binary, numeric, and categorical. 

**Binary Features:**
- **Definition:** Features with only two possible values (e.g., 0 or 1, Yes or No).
- **Encoding Best Practices:** Binary features are often left as-is if they are already in numeric format (0 and 1). If binary features are in text format (e.g., Yes/No), they should be encoded to 0 and 1 respectively.
- **Importance:** Proper handling ensures that models interpret these features correctly without additional transformation complexity.

**Numeric Features:**
- **Definition:** Features with quantitative values that can be measured.
- **Encoding Best Practices:** Numeric features may be scaled or normalized to ensure that no single feature dominates the model due to its scale.
- **Importance:** Scaling or normalizing numeric features helps in speeding up the learning process of algorithms and improves model performance.

**Categorical Features:**
- **Definition:** Features with a fixed number of categories or distinct groups.
- **Encoding Best Practices:** Categorical variables are typically encoded using techniques such as one-hot encoding, label encoding, or using embedding layers if using neural networks.
- **Importance:** Encoding transforms categorical data into a format that can be easily used by machine learning algorithms to better understand the patterns within the data.





## 3.2 Modeling
- **Model Selection:** Utilized LightGBM for its efficiency with large datasets and categorical features handling.
- **Parameter Tuning:** Conducted parameter tuning to optimize the LightGBM model, ensuring the best possible performance.

## 3.3 Validation Strategy
- Ensured a robust validation strategy that mimics real-world deployment rather than optimizing for leaderboard position, emphasizing the importance of model generalization.

# 4. Requirements
- **Python Version:** 3.7.1
- **Major Libraries:** pandas, numpy, lightgbm, scikit-learn

# 5. Usage
1. **Data Preparation:** Run the notebooks under `notebooks/` to clean and prepare data.
2. **Model Training:** Execute model training scripts to train and save the model.
3. **Evaluation:** Evaluate the model's performance on a separate validation set to ensure generalizability.

# 6. Reflections
Participating in this competition offered invaluable insights into the practical aspects of machine learning projects, including the importance of data quality, thoughtful feature engineering, and the need for robust validation methods.

# 7. Links
- [Competition Data](https://www.kaggle.com/c/microsoft-malware-prediction/data)
- [My Kaggle Profile](https://www.kaggle.com/yourusername)
