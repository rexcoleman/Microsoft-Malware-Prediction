# Data Science Portfolio: Cybersecurity Focus

## Table of Contents
1. [Overview](#1-overview)
2. [Key Lessons Learned](#2-key-lessons-learned)
3. [Solution Summary](#3-solution-summary)
    - [3.1 Data Preprocessing](#31-data-preprocessing)
        - [3.1.1 Introduction](#311-introduction)
        - [3.1.2 Data Cleaning](#312-data-cleaning)
        - [3.1.3 Feature Categorization and Encoding](#313-feature-categorization-and-encoding)
    - [3.2 Modeling](#32-modeling)
    - [3.3 Validation Strategy](#33-validation-strategy)
4. [Requirements](#4-requirements)
5. [Usage](#5-usage)
6. [Reflections](#6-reflections)
7. [Links](#7-links)

# 1. Overview
This repository documents my journey and learning from participating in the [Microsoft Malware Prediction competition on Kaggle](https://www.kaggle.com/c/microsoft-malware-prediction). The goal of this competition was to predict the probability that a machine running Windows 10 could be susceptible to malware attacks, based on various system features provided by Microsoft.

# 2. Key Lessons Learned
- **Emulating Real-World Solutions:** The approach focused on creating a model that could realistically be implemented in a production environment, not just optimized for competition scores.
- **Handling Large Datasets:** Developed strategies for managing and processing large datasets efficiently to prepare the data for modeling.

# 3. Solution Summary

## 3.1 Data Preprocessing

### 3.1.1 Introduction
Data preprocessing is a critical step in the data science workflow, especially in the context of cybersecurity. It involves cleaning and transforming raw data into a format that is suitable for analysis and modeling. This section will discuss various data preprocessing techniques used in this project, explaining their importance in a way that is accessible to non-technical business executives.

### 3.1.2 Data Cleaning
The Data Cleaning Process is essential for ensuring the quality and usability of the data. Here's an overview of the key steps involved and their significance:

#### Feature Removal
**Missing Values:** Features with too many NaN (missing) values are deleted because they can skew the analysis and might lead to unreliable models. A threshold of 90% missing values is generally considered as a criterion for removal, as it indicates that the feature is largely absent and likely not useful for predictive modeling.

<img src="img/features_with_high_nan.png" alt="nan_features" width="900"> 

*Figure 1: Missing Values - This table shows the featurew with the highest percentages of missing values.  For example: PuaMode and Census_ProcessorClass have 99%+ missing values.*

**Unbalanced Features:** Features with highly unbalanced dimensions are deleted. For example, a feature where a single category dominates can be less informative for modeling. A common threshold for considering a feature as unbalanced is when more than 95% of the values are concentrated in one category. This high concentration means the feature will not contribute significantly to the model's ability to distinguish between different observations.

Unbalanced Featues Table                              | Unbalanced IsBeta Feature
:---------------------------------------------------: | :---------------------------------------------------:
<img src="img/unbalanced_features.png" alt="HTML Table Target" width="450">  | <img src="img/umbalanced_feature_bar_chart.png" alt="HTML Table Target" width="610"> 

*Figure 2: Unbalanced Features - There are 26 columns in total in which one category contains > or = 90% of the values (left) and esentially 100% of IsBeta values belong to IsBeta (right). *

### 3.1.3 Feature Categorization and Encoding
Proper categorization and encoding of features are crucial as they directly influence the accuracy and efficiency of predictive models. They ensure that the algorithm correctly interprets the data, respects the nature of the data, and helps in achieving better performance. Features are split into three main types: binary, numeric, and categorical. Each type requires specific handling to ensure optimal data structure for machine learning algorithms:

#### General Data Type Encoding Guidance

**Binary Features:**
- **Definition:** Features with only two possible values (e.g., 0 or 1, Yes or No).
- **Encoding Best Practices:** Binary features are often left as-is if they are already in numeric format (0 and 1). If binary features are in text format (e.g., Yes/No), they should be encoded to 0 and 1 respectively.
- **Importance:** Proper handling ensures that models interpret these features correctly without additional transformation complexity.

**Numeric Features:**
- **Definition:** Features with quantitative values that can be measured.
- **Encoding Best Practices:** Numeric features may be scaled or normalized to ensure that no single feature dominates the model due to its scale.
- **Importance:** Scaling or normalizing numeric features helps in speeding up the learning process of algorithms and improves model performance.

**Categorical Features:**
- **Definition:** Features with a fixed number of categories or distinct groups.
- **Encoding Best Practices:** Categorical variables are typically encoded using techniques such as one-hot encoding, label encoding, or using embedding layers if using neural networks.
- **Importance:** Encoding transforms categorical data into a format that can be easily used by machine learning algorithms to better understand the patterns within the data.

#### Encoding Methods Used
**Label Encoding:** Transforms categorical feature labels into numeric form ranging from `0` to `N-1`, where `N` is the number of different labels. This is typically used for categorical features that have a natural ordinal relationship.

**Frequency Encoding:** A variant of label encoding where features are encoded based on the frequency of their values. The encoded numbers range from `0` to `N`, where `N` is the number of values that occur more than once. This method is effective for categorical data where the frequency of values may have a predictive power.


### 1. Label Encoding
- **Description:** Transforms categorical feature labels into numeric form ranging from `0` to `N-1`, where `N` is the number of different labels.
- **Application:** Used primarily for categorical features that have a natural ordinal relationship (where ordering is important).

### 2. Frequency Encoding
- **Description:** A variant of label encoding where features are encoded based on the frequency of their values. The encoded numbers range from `0` to `N`, where `N` is the number of values that occur more than once.
- **Application:** Effective for categorical data where the frequency of values may have a predictive power. For instance, frequently occurring values might have different characteristics compared to less frequent ones.



## 3.2 Modeling
- **Model Selection:** Utilized LightGBM for its efficiency with large datasets and categorical features handling.
- **Parameter Tuning:** Conducted parameter tuning to optimize the LightGBM model, ensuring the best possible performance.

## 3.3 Validation Strategy
- Ensured a robust validation strategy that mimics real-world deployment rather than optimizing for leaderboard position, emphasizing the importance of model generalization.

# 4. Requirements
- **Python Version:** 3.7.1
- **Major Libraries:** pandas, numpy, lightgbm, scikit-learn

# 5. Usage
1. **Data Preparation:** Run the notebooks under `notebooks/` to clean and prepare data.
2. **Model Training:** Execute model training scripts to train and save the model.
3. **Evaluation:** Evaluate the model's performance on a separate validation set to ensure generalizability.

# 6. Reflections
Participating in this competition offered invaluable insights into the practical aspects of machine learning projects, including the importance of data quality, thoughtful feature engineering, and the need for robust validation methods.

# 7. Links
- [Competition Data](https://www.kaggle.com/c/microsoft-malware-prediction/data)
- [My Kaggle Profile](https://www.kaggle.com/yourusername)

#8. Appendix

## 8.2 Appendix 2: Common Encoding Methods and Their Applications in Cybersecurity

This section outlines various encoding methods for categorical data, their advantages, disadvantages, and examples of applications in cybersecurity. Choosing the correct encoding technique is crucial as it can greatly affect the performance of machine learning models.

### 1. **Label Encoding**
- **Description:** Assigns a unique integer to each category based on alphabetical order.
- **Pros:** Efficient and simple.
- **Cons:** Implies an ordinal relationship which may not exist.
- **Cybersecurity Example:** Encoding threat levels (Low, Medium, High) in security logs.

### 2. **Frequency Encoding**
- **Description:** Replaces categories with their occurrence counts.
- **Pros:** Keeps information about category frequency.
- **Cons:** Can merge different categories if they have the same frequency.
- **Cybersecurity Example:** Encoding frequency of access to sensitive system resources.

### 3. **One-Hot Encoding**
- **Description:** Creates a new binary column for each category.
- **Pros:** Does not assume ordinality between categories.
- **Cons:** Increases dataset dimensionality significantly.
- **Cybersecurity Example:** Encoding HTTP methods (GET, POST, DELETE) in web traffic data.

### 4. **Binary Encoding**
- **Description:** Converts categories into binary codes.
- **Pros:** More compact than one-hot encoding.
- **Cons:** Introduces multiple columns.
- **Cybersecurity Example:** Encoding network protocol types.

### 5. **Hashing**
- **Description:** Uses a hash function to encode categories into integers.
- **Pros:** Efficient with high cardinality features.
- **Cons:** Potential hash collisions.
- **Cybersecurity Example:** Anonymizing IP addresses in large datasets.

### 6. **BaseN Encoding**
- **Description:** Generalization of binary encoding using any base N.
- **Pros:** More flexible and efficient than one-hot encoding.
- **Cons:** Complexity can increase with larger N.
- **Cybersecurity Example:** Encoding software version numbers.

### 7. **Mean (Target) Encoding**
- **Description:** Replaces categories with the average value of the target for that category.
- **Pros:** Can improve model performance.
- **Cons:** Risk of overfitting.
- **Cybersecurity Example:** Encoding country codes in fraud detection systems based on fraud rates.

### 8. **Weight of Evidence Encoding**
- **Description:** Quantifies the predictive power of a category with respect to the target.
- **Pros:** Provides interpretable encoding.
- **Cons:** Biased by rare categories.
- **Cybersecurity Example:** Encoding user roles for predicting security policy violations.

### 9. **Ordinal Encoding**
- **Description:** Converts categories to integers based on the order.
- **Pros:** Minimal feature expansion.
- **Cons:** Assumes an order that might not exist.
- **Cybersecurity Example:** Encoding risk ratings from security tools.

### 10. **Leave-One-Out Encoding**
- **Description:** Similar to target encoding but reduces overfitting by excluding the current row's category while calculating the mean.
- **Pros:** Reduces overfitting risk.
- **Cons:** Computationally intensive.
- **Cybersecurity Example:** Encoding asset tags when assessing compromise risks.

### 11. **Backward Difference Encoding**
- **Description:** Compares the mean of the dependent variable for one level to the mean of the previous level.
- **Pros:** Useful for ordinal data with linear relationships.
- **Cons:** Assumes linear relationships.
- **Cybersecurity Example:** Encoding levels of security software upgrades.

### 12. **Helmert Encoding**
- **Description:** Compares each level of a categorical variable to the mean of the subsequent levels.
- **Pros:** Does not assume a starting point.
- **Cons:** Complex and harder to interpret.
- **Cybersecurity Example:** Useful in experimental designs in cybersecurity studies.

## Conclusion
The selection of an encoding method should consider the specific characteristics of the data, the machine learning model requirements, and the particular domain of application. In cybersecurity, where accuracy and interpretability are crucial, the right choice of encoding can significantly enhance model performance.
